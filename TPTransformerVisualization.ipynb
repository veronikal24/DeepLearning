{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8cd5aa2",
   "metadata": {},
   "source": [
    "# Visualization of Transformer Prediction on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b1d4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tptrans import TPTrans\n",
    "from dataloader import load_parquet, preprocess_data, SlidingWindowDataset\n",
    "from plotting import plot_testresult_sample\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca442887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in your case it will be\n",
    "#dataset = \"dataset\"\n",
    "dataset = \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d97449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/63/7/219953/DeepLearning-1/dataloader.py:167: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df.set_index(\"Timestamp\")\n",
      "/zhome/63/7/219953/DeepLearning-1/dataloader.py:167: FutureWarning: DataFrameGroupBy.resample operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.set_index(\"Timestamp\")\n",
      "/zhome/63/7/219953/.local/lib/python3.9/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TPTrans:\n\tMissing key(s) in state_dict: \"input_proj.weight\", \"input_proj.bias\", \"encoder.layers.4.self_attn.in_proj_weight\", \"encoder.layers.4.self_attn.in_proj_bias\", \"encoder.layers.4.self_attn.out_proj.weight\", \"encoder.layers.4.self_attn.out_proj.bias\", \"encoder.layers.4.linear1.weight\", \"encoder.layers.4.linear1.bias\", \"encoder.layers.4.linear2.weight\", \"encoder.layers.4.linear2.bias\", \"encoder.layers.4.norm1.weight\", \"encoder.layers.4.norm1.bias\", \"encoder.layers.4.norm2.weight\", \"encoder.layers.4.norm2.bias\", \"encoder.layers.5.self_attn.in_proj_weight\", \"encoder.layers.5.self_attn.in_proj_bias\", \"encoder.layers.5.self_attn.out_proj.weight\", \"encoder.layers.5.self_attn.out_proj.bias\", \"encoder.layers.5.linear1.weight\", \"encoder.layers.5.linear1.bias\", \"encoder.layers.5.linear2.weight\", \"encoder.layers.5.linear2.bias\", \"encoder.layers.5.norm1.weight\", \"encoder.layers.5.norm1.bias\", \"encoder.layers.5.norm2.weight\", \"encoder.layers.5.norm2.bias\", \"encoder.layers.6.self_attn.in_proj_weight\", \"encoder.layers.6.self_attn.in_proj_bias\", \"encoder.layers.6.self_attn.out_proj.weight\", \"encoder.layers.6.self_attn.out_proj.bias\", \"encoder.layers.6.linear1.weight\", \"encoder.layers.6.linear1.bias\", \"encoder.layers.6.linear2.weight\", \"encoder.layers.6.linear2.bias\", \"encoder.layers.6.norm1.weight\", \"encoder.layers.6.norm1.bias\", \"encoder.layers.6.norm2.weight\", \"encoder.layers.6.norm2.bias\", \"encoder.layers.7.self_attn.in_proj_weight\", \"encoder.layers.7.self_attn.in_proj_bias\", \"encoder.layers.7.self_attn.out_proj.weight\", \"encoder.layers.7.self_attn.out_proj.bias\", \"encoder.layers.7.linear1.weight\", \"encoder.layers.7.linear1.bias\", \"encoder.layers.7.linear2.weight\", \"encoder.layers.7.linear2.bias\", \"encoder.layers.7.norm1.weight\", \"encoder.layers.7.norm1.bias\", \"encoder.layers.7.norm2.weight\", \"encoder.layers.7.norm2.bias\", \"decoder_delta_lat.0.weight\", \"decoder_delta_lat.0.bias\", \"decoder_delta_lat.2.weight\", \"decoder_delta_lat.2.bias\", \"decoder_delta_lon.0.weight\", \"decoder_delta_lon.0.bias\", \"decoder_delta_lon.2.weight\", \"decoder_delta_lon.2.bias\". \n\tUnexpected key(s) in state_dict: \"decoder_input_proj.weight\", \"decoder_input_proj.bias\", \"decoder.layers.0.self_attn.in_proj_weight\", \"decoder.layers.0.self_attn.in_proj_bias\", \"decoder.layers.0.self_attn.out_proj.weight\", \"decoder.layers.0.self_attn.out_proj.bias\", \"decoder.layers.0.multihead_attn.in_proj_weight\", \"decoder.layers.0.multihead_attn.in_proj_bias\", \"decoder.layers.0.multihead_attn.out_proj.weight\", \"decoder.layers.0.multihead_attn.out_proj.bias\", \"decoder.layers.0.linear1.weight\", \"decoder.layers.0.linear1.bias\", \"decoder.layers.0.linear2.weight\", \"decoder.layers.0.linear2.bias\", \"decoder.layers.0.norm1.weight\", \"decoder.layers.0.norm1.bias\", \"decoder.layers.0.norm2.weight\", \"decoder.layers.0.norm2.bias\", \"decoder.layers.0.norm3.weight\", \"decoder.layers.0.norm3.bias\", \"decoder.layers.1.self_attn.in_proj_weight\", \"decoder.layers.1.self_attn.in_proj_bias\", \"decoder.layers.1.self_attn.out_proj.weight\", \"decoder.layers.1.self_attn.out_proj.bias\", \"decoder.layers.1.multihead_attn.in_proj_weight\", \"decoder.layers.1.multihead_attn.in_proj_bias\", \"decoder.layers.1.multihead_attn.out_proj.weight\", \"decoder.layers.1.multihead_attn.out_proj.bias\", \"decoder.layers.1.linear1.weight\", \"decoder.layers.1.linear1.bias\", \"decoder.layers.1.linear2.weight\", \"decoder.layers.1.linear2.bias\", \"decoder.layers.1.norm1.weight\", \"decoder.layers.1.norm1.bias\", \"decoder.layers.1.norm2.weight\", \"decoder.layers.1.norm2.bias\", \"decoder.layers.1.norm3.weight\", \"decoder.layers.1.norm3.bias\", \"decoder.layers.2.self_attn.in_proj_weight\", \"decoder.layers.2.self_attn.in_proj_bias\", \"decoder.layers.2.self_attn.out_proj.weight\", \"decoder.layers.2.self_attn.out_proj.bias\", \"decoder.layers.2.multihead_attn.in_proj_weight\", \"decoder.layers.2.multihead_attn.in_proj_bias\", \"decoder.layers.2.multihead_attn.out_proj.weight\", \"decoder.layers.2.multihead_attn.out_proj.bias\", \"decoder.layers.2.linear1.weight\", \"decoder.layers.2.linear1.bias\", \"decoder.layers.2.linear2.weight\", \"decoder.layers.2.linear2.bias\", \"decoder.layers.2.norm1.weight\", \"decoder.layers.2.norm1.bias\", \"decoder.layers.2.norm2.weight\", \"decoder.layers.2.norm2.bias\", \"decoder.layers.2.norm3.weight\", \"decoder.layers.2.norm3.bias\", \"decoder.layers.3.self_attn.in_proj_weight\", \"decoder.layers.3.self_attn.in_proj_bias\", \"decoder.layers.3.self_attn.out_proj.weight\", \"decoder.layers.3.self_attn.out_proj.bias\", \"decoder.layers.3.multihead_attn.in_proj_weight\", \"decoder.layers.3.multihead_attn.in_proj_bias\", \"decoder.layers.3.multihead_attn.out_proj.weight\", \"decoder.layers.3.multihead_attn.out_proj.bias\", \"decoder.layers.3.linear1.weight\", \"decoder.layers.3.linear1.bias\", \"decoder.layers.3.linear2.weight\", \"decoder.layers.3.linear2.bias\", \"decoder.layers.3.norm1.weight\", \"decoder.layers.3.norm1.bias\", \"decoder.layers.3.norm2.weight\", \"decoder.layers.3.norm2.bias\", \"decoder.layers.3.norm3.weight\", \"decoder.layers.3.norm3.bias\", \"output_head.weight\", \"output_head.bias\". \n\tsize mismatch for pos_encoder.pe: copying a param with shape torch.Size([1, 500, 512]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for conv.weight: copying a param with shape torch.Size([512, 6, 5]) from checkpoint, the shape in current model is torch.Size([256, 256, 3]).\n\tsize mismatch for conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.linear1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for encoder.layers.0.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.layers.0.linear2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for encoder.layers.0.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.linear1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for encoder.layers.1.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.layers.1.linear2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for encoder.layers.1.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.linear1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for encoder.layers.2.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.layers.2.linear2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for encoder.layers.2.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.3.self_attn.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.layers.3.self_attn.in_proj_bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.3.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.layers.3.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.3.linear1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for encoder.layers.3.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.layers.3.linear2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for encoder.layers.3.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.3.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.3.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.3.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.3.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for pos_decoder.pe: copying a param with shape torch.Size([1, 500, 512]) from checkpoint, the shape in current model is torch.Size([2048, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m TPTrans(pred_len\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints/tptrans_delta_lin_newNewParams_375.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#model.load_state_dict(torch.load(\"/zhome/63/7/219953/DeepLearning-1/checkpoints/tpinformer_delta_lin_newNewParams_375.pth\", map_location=torch.device(device)))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:2624\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2616\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2617\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2618\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2619\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2620\u001b[0m             ),\n\u001b[1;32m   2621\u001b[0m         )\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2626\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2627\u001b[0m         )\n\u001b[1;32m   2628\u001b[0m     )\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TPTrans:\n\tMissing key(s) in state_dict: \"input_proj.weight\", \"input_proj.bias\", \"encoder.layers.4.self_attn.in_proj_weight\", \"encoder.layers.4.self_attn.in_proj_bias\", \"encoder.layers.4.self_attn.out_proj.weight\", \"encoder.layers.4.self_attn.out_proj.bias\", \"encoder.layers.4.linear1.weight\", \"encoder.layers.4.linear1.bias\", \"encoder.layers.4.linear2.weight\", \"encoder.layers.4.linear2.bias\", \"encoder.layers.4.norm1.weight\", \"encoder.layers.4.norm1.bias\", \"encoder.layers.4.norm2.weight\", \"encoder.layers.4.norm2.bias\", \"encoder.layers.5.self_attn.in_proj_weight\", \"encoder.layers.5.self_attn.in_proj_bias\", \"encoder.layers.5.self_attn.out_proj.weight\", \"encoder.layers.5.self_attn.out_proj.bias\", \"encoder.layers.5.linear1.weight\", \"encoder.layers.5.linear1.bias\", \"encoder.layers.5.linear2.weight\", \"encoder.layers.5.linear2.bias\", \"encoder.layers.5.norm1.weight\", \"encoder.layers.5.norm1.bias\", \"encoder.layers.5.norm2.weight\", \"encoder.layers.5.norm2.bias\", \"encoder.layers.6.self_attn.in_proj_weight\", \"encoder.layers.6.self_attn.in_proj_bias\", \"encoder.layers.6.self_attn.out_proj.weight\", \"encoder.layers.6.self_attn.out_proj.bias\", \"encoder.layers.6.linear1.weight\", \"encoder.layers.6.linear1.bias\", \"encoder.layers.6.linear2.weight\", \"encoder.layers.6.linear2.bias\", \"encoder.layers.6.norm1.weight\", \"encoder.layers.6.norm1.bias\", \"encoder.layers.6.norm2.weight\", \"encoder.layers.6.norm2.bias\", \"encoder.layers.7.self_attn.in_proj_weight\", \"encoder.layers.7.self_attn.in_proj_bias\", \"encoder.layers.7.self_attn.out_proj.weight\", \"encoder.layers.7.self_attn.out_proj.bias\", \"encoder.layers.7.linear1.weight\", \"encoder.layers.7.linear1.bias\", \"encoder.layers.7.linear2.weight\", \"encoder.layers.7.linear2.bias\", \"encoder.layers.7.norm1.weight\", \"encoder.layers.7.norm1.bias\", \"encoder.layers.7.norm2.weight\", \"encoder.layers.7.norm2.bias\", \"decoder_delta_lat.0.weight\", \"decoder_delta_lat.0.bias\", \"decoder_delta_lat.2.weight\", \"decoder_delta_lat.2.bias\", \"decoder_delta_lon.0.weight\", \"decoder_delta_lon.0.bias\", \"decoder_delta_lon.2.weight\", \"decoder_delta_lon.2.bias\". \n\tUnexpected key(s) in state_dict: \"decoder_input_proj.weight\", \"decoder_input_proj.bias\", \"decoder.layers.0.self_attn.in_proj_weight\", \"decoder.layers.0.self_attn.in_proj_bias\", \"decoder.layers.0.self_attn.out_proj.weight\", \"decoder.layers.0.self_attn.out_proj.bias\", \"decoder.layers.0.multihead_attn.in_proj_weight\", \"decoder.layers.0.multihead_attn.in_proj_bias\", \"decoder.layers.0.multihead_attn.out_proj.weight\", \"decoder.layers.0.multihead_attn.out_proj.bias\", \"decoder.layers.0.linear1.weight\", \"decoder.layers.0.linear1.bias\", \"decoder.layers.0.linear2.weight\", \"decoder.layers.0.linear2.bias\", \"decoder.layers.0.norm1.weight\", \"decoder.layers.0.norm1.bias\", \"decoder.layers.0.norm2.weight\", \"decoder.layers.0.norm2.bias\", \"decoder.layers.0.norm3.weight\", \"decoder.layers.0.norm3.bias\", \"decoder.layers.1.self_attn.in_proj_weight\", \"decoder.layers.1.self_attn.in_proj_bias\", \"decoder.layers.1.self_attn.out_proj.weight\", \"decoder.layers.1.self_attn.out_proj.bias\", \"decoder.layers.1.multihead_attn.in_proj_weight\", \"decoder.layers.1.multihead_attn.in_proj_bias\", \"decoder.layers.1.multihead_attn.out_proj.weight\", \"decoder.layers.1.multihead_attn.out_proj.bias\", \"decoder.layers.1.linear1.weight\", \"decoder.layers.1.linear1.bias\", \"decoder.layers.1.linear2.weight\", \"decoder.layers.1.linear2.bias\", \"decoder.layers.1.norm1.weight\", \"decoder.layers.1.norm1.bias\", \"decoder.layers.1.norm2.weight\", \"decoder.layers.1.norm2.bias\", \"decoder.layers.1.norm3.weight\", \"decoder.layers.1.norm3.bias\", \"decoder.layers.2.self_attn.in_proj_weight\", \"decoder.layers.2.self_attn.in_proj_bias\", \"decoder.layers.2.self_attn.out_proj.weight\", \"decoder.layers.2.self_attn.out_proj.bias\", \"decoder.layers.2.multihead_attn.in_proj_weight\", \"decoder.layers.2.multihead_attn.in_proj_bias\", \"decoder.layers.2.multihead_attn.out_proj.weight\", \"decoder.layers.2.multihead_attn.out_proj.bias\", \"decoder.layers.2.linear1.weight\", \"decoder.layers.2.linear1.bias\", \"decoder.layers.2.linear2.weight\", \"decoder.layers.2.linear2.bias\", \"decoder.layers.2.norm1.weight\", \"decoder.layers.2.norm1.bias\", \"decoder.layers.2.norm2.weight\", \"decoder.layers.2.norm2.bias\", \"decoder.layers.2.norm3.weight\", \"decoder.layers.2.norm3.bias\", \"decoder.layers.3.self_attn.in_proj_weight\", \"decoder.layers.3.self_attn.in_proj_bias\", \"decoder.layers.3.self_attn.out_proj.weight\", \"decoder.layers.3.self_attn.out_proj.bias\", \"decoder.layers.3.multihead_attn.in_proj_weight\", \"decoder.layers.3.multihead_attn.in_proj_bias\", \"decoder.layers.3.multihead_attn.out_proj.weight\", \"decoder.layers.3.multihead_attn.out_proj.bias\", \"decoder.layers.3.linear1.weight\", \"decoder.layers.3.linear1.bias\", \"decoder.layers.3.linear2.weight\", \"decoder.layers.3.linear2.bias\", \"decoder.layers.3.norm1.weight\", \"decoder.layers.3.norm1.bias\", \"decoder.layers.3.norm2.weight\", \"decoder.layers.3.norm2.bias\", \"decoder.layers.3.norm3.weight\", \"decoder.layers.3.norm3.bias\", \"output_head.weight\", \"output_head.bias\". \n\tsize mismatch for pos_encoder.pe: copying a param with shape torch.Size([1, 500, 512]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for conv.weight: copying a param with shape torch.Size([512, 6, 5]) from checkpoint, the shape in current model is torch.Size([256, 256, 3]).\n\tsize mismatch for conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.linear1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for encoder.layers.0.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.layers.0.linear2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for encoder.layers.0.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.linear1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for encoder.layers.1.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.layers.1.linear2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for encoder.layers.1.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.linear1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for encoder.layers.2.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.layers.2.linear2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for encoder.layers.2.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.3.self_attn.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for encoder.layers.3.self_attn.in_proj_bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.3.self_attn.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for encoder.layers.3.self_attn.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.3.linear1.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for encoder.layers.3.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.layers.3.linear2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for encoder.layers.3.linear2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.3.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.3.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.3.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.3.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for pos_decoder.pe: copying a param with shape torch.Size([1, 500, 512]) from checkpoint, the shape in current model is torch.Size([2048, 256])."
     ]
    }
   ],
   "source": [
    "df = load_parquet(dataset, k=5)\n",
    "df = preprocess_data(df)\n",
    "\n",
    "dataset = SlidingWindowDataset(\n",
    "    df,\n",
    "    max_diff_per_sequence_minutes=15,\n",
    "    window_size_minutes=120,\n",
    "    pred_size_minutes=30,\n",
    "    stride=15,\n",
    ")\n",
    "test_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TPTrans(pred_len=dataset[0][1].shape[0]).to(device)\n",
    "#\n",
    "model.load_state_dict(torch.load(\"checkpoints/tptrans_delta_lin_newNewParams_375.pth\", map_location=torch.device(device)))\n",
    "#model.load_state_dict(torch.load(\"/zhome/63/7/219953/DeepLearning-1/checkpoints/tpinformer_delta_lin_newNewParams_375.pth\", map_location=torch.device(device)))\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "test_loss = 0.0\n",
    "data = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        tgt_start = x[:, -1:, :2]  # (batch, 1, 2) lat/lon\n",
    "        pred = model(x, tgt_start)\n",
    "        data.append((x,y,pred))\n",
    "        loss = criterion(pred, y)\n",
    "        test_loss += loss.item() * x.size(0)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f\"Test Loss: {test_loss:.6f}\", flush=True)\n",
    "\n",
    "plot_testresult_sample(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
